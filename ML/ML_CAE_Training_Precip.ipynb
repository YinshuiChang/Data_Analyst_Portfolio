{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64882817-340c-457a-8a4a-ff906e27a4d0",
   "metadata": {},
   "source": [
    "# Convolutional Autoencoder (CAE)\n",
    "A Convolutional Autoencoder (CAE) is a type of deep neural network used primarily for unsupervised learning tasks, such as image reconstruction and feature learning. It combines the principles of convolutional neural networks (CNNs) and autoencoders to efficiently encode and decode data, particularly images.\n",
    "Key Components of a Convolutional Autoencoder:\n",
    "### Encoder:\n",
    "* **Convolutional Layers:** The encoder uses convolutional layers to capture spatial hierarchies in the input data. Convolutional layers apply filters to the input, generating feature maps that highlight different aspects of the data.\n",
    "* **Pooling Layers:** Often, pooling layers (e.g., max pooling) are used to downsample the feature maps, reducing their dimensions and retaining essential features.\n",
    "* **Latent Space Representation:** The encoder compresses the input data into a lower-dimensional latent space representation. This compressed form captures the most important information of the input data.\n",
    "### Decoder:\n",
    "* **Transposed Convolutional Layers:** The decoder mirrors the encoder but uses transposed convolutional (or deconvolutional) layers to upsample the latent representation back to the original input size.\n",
    "* **Reconstruction:** The decoder reconstructs the input data from its compressed form, ideally producing an output that is as close as possible to the original input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0f5257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses\n",
    "\n",
    "from Models import CNN_AE_vx01 as ae\n",
    "ae_version = ae.version\n",
    "ae_layers = ae.nlayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "569fdc11-3aef-4e89-a2a4-58762b760808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.9\n",
      "numpy: 1.23.5\n",
      "tensorflow: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "import sys\n",
    "print('Python: ' + python_version()) # Python: 3.10.9\n",
    "print('numpy: ' + np.__version__) # numpy: 1.23.5\n",
    "print ('tensorflow: ' + sys.modules[\"tensorflow\"].__version__) # tensorflow: 2.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10705ecc-9c2e-4ccd-b324-0f1cb7c0c115",
   "metadata": {},
   "source": [
    "# Initialization of the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "640c25ea-1190-4a9d-bcd8-6b9d1845a081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder = CNN_AE()\n",
    "autoencoder = ae.CNN_AE()\n",
    "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7e3e865-5976-4e67-b87d-bfdc854a8679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 90, 180, 16)       160       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 45, 90, 16)        2320      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 45, 90, 1)         145       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,625\n",
      "Trainable params: 2,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_transpose (Conv2DTra  (None, 90, 180, 16)      160       \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 180, 360, 16)     2320      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 180, 360, 1)       145       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,625\n",
      "Trainable params: 2,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"cnn_ae\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 45, 90, 1)         2625      \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (None, 180, 360, 1)       2625      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,250\n",
      "Trainable params: 5,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.build((None,182, 362, ae_layers))\n",
    "autoencoder.encoder.summary()\n",
    "autoencoder.decoder.summary()\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5700fd4f-cdee-4cd3-8af3-50efc798b94a",
   "metadata": {},
   "source": [
    "# Load autoencoder from weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5336f56a-4d64-4c73-ae05-7b58bed6180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "version = \"34\"\n",
    "autoencoder = ae.CNN_AE()\n",
    "autoencoder.encoder.load_weights('./Weights/cnn_ae_v' + ae_version + '_encoder_weights_' + version)\n",
    "autoencoder.decoder.load_weights('./Weights/cnn_ae_v' + ae_version + '_decoder_weights_' + version)\n",
    "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "autoencoder.build((None,182, 362, ae_layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59450ca1-ada1-4b22-8d91-1fb9a3c1ab27",
   "metadata": {},
   "source": [
    "# Load training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcf352fb-0f30-4fa6-8a26-54b765c8c256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9232, 182, 362, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if ae_layers == 1:\n",
    "    datacube_precip = np.load(\"../data/WaterPrecip_datacube_CAE_single.npy\")\n",
    "else:\n",
    "    datacube_precip = np.load(\"../data/WaterPrecip_datacube_CNN_x.npy\")\n",
    "    \n",
    "datacube_precip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da5ed21a-9acd-4c23-b702-87729a8a7d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 180, 360, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nTest = 1000\n",
    "iTest = np.random.choice(datacube_precip.shape[0],nTest, replace=False)\n",
    "iTrain = [i for i in range(datacube_precip.shape[0]) if i not in iTest]\n",
    "datacube_precip[iTest,1:181,1:361,:].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab086ebc-5e2e-4670-9823-7d4366a20a14",
   "metadata": {},
   "source": [
    "# Train Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f38be47a-148b-4025-92bb-f370c6607d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "258/258 [==============================] - 84s 327ms/step - loss: 3.9944 - val_loss: 4.0214\n",
      "Epoch 2/10\n",
      "258/258 [==============================] - 82s 316ms/step - loss: 3.9875 - val_loss: 4.0233\n",
      "Epoch 3/10\n",
      "258/258 [==============================] - 87s 338ms/step - loss: 3.9820 - val_loss: 4.0081\n",
      "Epoch 4/10\n",
      "258/258 [==============================] - 84s 325ms/step - loss: 3.9745 - val_loss: 4.0022\n",
      "Epoch 5/10\n",
      "258/258 [==============================] - 88s 341ms/step - loss: 3.9706 - val_loss: 3.9976\n",
      "Epoch 6/10\n",
      "258/258 [==============================] - 79s 305ms/step - loss: 3.9632 - val_loss: 3.9903\n",
      "Epoch 7/10\n",
      "258/258 [==============================] - 79s 306ms/step - loss: 3.9592 - val_loss: 3.9856\n",
      "Epoch 8/10\n",
      "258/258 [==============================] - 83s 321ms/step - loss: 3.9551 - val_loss: 3.9821\n",
      "Epoch 9/10\n",
      "258/258 [==============================] - 80s 312ms/step - loss: 3.9497 - val_loss: 3.9778\n",
      "Epoch 10/10\n",
      "258/258 [==============================] - 84s 325ms/step - loss: 3.9450 - val_loss: 3.9841\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29607eecd60>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(datacube_precip[iTrain,:,:,:ae_layers], datacube_precip[iTrain,1:181,1:361,:ae_layers],\n",
    "               epochs=10,\n",
    "               shuffle=True,\n",
    "               validation_data=(datacube_precip[iTest,:,:,:ae_layers], datacube_precip[iTest,1:181,1:361,:ae_layers]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc7ba77-17e6-4944-bda9-e4fa57b4522b",
   "metadata": {},
   "source": [
    "# Save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4094db6-1a40-4e4f-a97f-b3464f972b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"43\"\n",
    "autoencoder.encoder.save_weights('./Weights/cnn_ae_v' + ae_version + '_encoder_weights_' + version)\n",
    "autoencoder.decoder.save_weights('./Weights/cnn_ae_v' + ae_version + '_decoder_weights_' + version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f98821-c7e9-4537-aa02-e33fac8eaa78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
